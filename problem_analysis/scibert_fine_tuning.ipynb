{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune a SciBERT Model\n",
    "### Load queried data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\albbl\\Documents\\Studium\\11_Semester\\medtextclassification_repo\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "RANDOM_SEED = 42\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import ElementTree\n",
    "import pandas as pd\n",
    "\n",
    "# Extract data from XML and create a DataFrame\n",
    "xml_files = [\"NEJM_data.xml\", \"animals_data.xml\"]\n",
    "data_path = \"../data-querying/results/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "tree = ElementTree()\n",
    "hum_xml = tree.parse(data_path + xml_files[0])\n",
    "for i, rec in enumerate(hum_xml.findall('.//Rec')):\n",
    "    try: \n",
    "        common = rec.find('.//Common')\n",
    "        pmid = common.find('PMID').text\n",
    "        title = common.find('Title').text\n",
    "        abstract = common.find('Abstract').text\n",
    "        mesh_term_list = rec.find('.//MeshTermList')\n",
    "        mesh_terms = [term.text for term in mesh_term_list.findall('MeshTerm')]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Error occured for PMID: {pmid}\")\n",
    "\n",
    "    data.append({'pmid': pmid, 'title': title,\n",
    "                'abstract': abstract, 'meshtermlist': mesh_terms, 'labels': [1,0]})\n",
    "    if i > 200:\n",
    "        break\n",
    "\n",
    "vet_xml = tree.parse(data_path + xml_files[1])\n",
    "for i, rec in enumerate(vet_xml.findall('.//Rec')):\n",
    "    try: \n",
    "        common = rec.find('.//Common')\n",
    "        pmid = common.find('PMID').text\n",
    "        title = common.find('Title').text\n",
    "        abstract = common.find('Abstract').text\n",
    "        mesh_term_list = rec.find('.//MeshTermList')\n",
    "        mesh_terms = [term.text for term in mesh_term_list.findall('MeshTerm')]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Error occured for PMID: {pmid}\")\n",
    "    data.append({'pmid': pmid, 'title': title,\n",
    "                'abstract': abstract, 'meshtermlist': mesh_terms, 'labels': [0,1]})\n",
    "    if i > 200:\n",
    "        break\n",
    "\n",
    "full_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        text = str(self.texts[idx])\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encodings = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        ids = encodings['input_ids'].flatten()\n",
    "        mask = encodings['attention_mask'].flatten()\n",
    "        token_type_ids = encodings[\"token_type_ids\"].flatten()\n",
    "\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': ids.to(device),\n",
    "            'attention_mask': mask.to(device),\n",
    "            'token_type_ids': token_type_ids.to(device),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (404, 5)\n",
      "TRAIN Dataset: (290, 5)\n",
      "VAL Dataset: (73, 5)\n",
      "TEST Dataset: (41, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "  full_df,\n",
    "  test_size=0.1,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_set, val_set = train_test_split(\n",
    "  train_set,\n",
    "  test_size=0.2,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "val_set.reset_index(drop=True, inplace=True)\n",
    "test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(full_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_set.shape))\n",
    "print(\"VAL Dataset: {}\".format(val_set.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(texts, targets, tokenizer, batch_size, max_len, num_workers=0):\n",
    "    dataset = Dataset(texts.to_numpy(), targets, tokenizer, max_len)\n",
    "    params = {\n",
    "        \"batch_size\":batch_size,\n",
    "        \"num_workers\":num_workers\n",
    "    }\n",
    "    dataloader = DataLoader(dataset, **params)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader(train_set.abstract, train_set.labels, tokenizer, TRAIN_BATCH_SIZE, MAX_LEN)\n",
    "val_dataloader = get_dataloader(val_set.abstract, val_set.labels, tokenizer, VAL_BATCH_SIZE, MAX_LEN)\n",
    "test_dataloader = get_dataloader(test_set.abstract, test_set.labels, tokenizer, TEST_BATCH_SIZE, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciBertClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(SciBertClassifier, self).__init__()\n",
    "\n",
    "        self.scibert =AutoModel.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear = torch.nn.Linear(self.scibert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        scibert_output = self.scibert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids = token_type_ids)\n",
    "        dropout_output = self.dropout(scibert_output[1])\n",
    "        output = self.linear(dropout_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SciBertClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, loss_fn, device):\n",
    "    model = model.eval()\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input_ids = data[\"input_ids\"].to(device, dtype = torch.long)\n",
    "            attention_mask = data[\"attention_mask\"].to(device, dtype = torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype = torch.long)\n",
    "            labels = data[\"labels\"].to(device, dtype = torch.float)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss += loss_fn(outputs, labels).item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == torch.argmax(labels, dim=1))\n",
    "            \n",
    "            \n",
    "    num_data = len(dataloader) * VAL_BATCH_SIZE\n",
    "    return correct_predictions / num_data, loss / num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, optimizer, device, scheduler, epochs):\n",
    "    progress_bar = tqdm(range(len(train_dataloader) * epochs))\n",
    "    model = model.train()\n",
    "    history = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        print(\"_\" * 30)\n",
    "        print(f'Epoch {epoch_num} started.')\n",
    "        \n",
    "        total_loss = 0\n",
    "        correct_predictions = 0.0\n",
    "        \n",
    "        for data in train_dataloader:\n",
    "            input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            labels = data['labels'].to(device, dtype = torch.float)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            #print(correct_predictions)\n",
    "            #print(outputs)\n",
    "            #print(preds)\n",
    "            correct_predictions += torch.sum(preds == torch.argmax(labels, dim=1)).item()\n",
    "            #print(labels)\n",
    "            #print(torch.argmax(labels, dim=1))\n",
    "            #print(correct_predictions)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            # to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        num_data = len(train_dataloader) * TRAIN_BATCH_SIZE\n",
    "        train_acc = correct_predictions / num_data\n",
    "        train_loss = total_loss / num_data\n",
    "        print(f'Epoch: {epoch_num}, Train Accuracy {train_acc}, Loss:  {train_loss}')\n",
    "\n",
    "        val_acc, val_loss = eval_model(model, val_dataloader, loss_fn, device)\n",
    "        print(f'Epoch: {epoch_num}, Validation Accuracy {val_acc}, Loss:  {val_loss}')\n",
    "        \n",
    "        history.append({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss})\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            torch.save(model.state_dict(), 'best_model.bin')\n",
    "            best_acc = val_acc\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/73 [00:00<?, ?it/s]c:\\Users\\albbl\\Documents\\Studium\\11_Semester\\medtextclassification_repo\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Epoch 0 started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [32:20<00:00, 25.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Accuracy 0.8287671232876712, Loss:  0.11658732654297188\n",
      "Epoch: 0, Validation Accuracy 0.9473684430122375, Loss:  0.06357408197302568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [33:33<00:00, 27.58s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "history = train(model, train_dataloader, val_dataloader, loss_fn, optimizer, device, scheduler, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts, tokenizer, max_len=512):\n",
    "    predictions = []\n",
    "    for _, data in enumerate(texts, 0):\n",
    "        text = str(data)\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = torch.tensor(inputs['input_ids'], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        token_type_ids = torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(ids, mask, token_type_ids)\n",
    "        \n",
    "        probabilities = torch.sigmoid(logits.squeeze())\n",
    "        predictions.append(probabilities)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST dataset - Accuracy: 0.9090909361839294, Loss: 0.06465540013530037\n"
     ]
    }
   ],
   "source": [
    "acc, loss = eval_model(model, test_dataloader, loss_fn, device)\n",
    "print(f\"TEST dataset - Accuracy: {acc}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.5749, 0.3904], device='cuda:0')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"A boy came with a leg injury.\"]\n",
    "\n",
    "predict(model, text, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
